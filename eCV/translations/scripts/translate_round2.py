#!/usr/bin/env python3
"""
Round 2 Translation - Focus on complete sentences, messages with parameters, and remaining content
"""

import xml.etree.ElementTree as ET
import re

# Round 2: Complete sentences, messages, and specific phrases
ROUND2_TRANSLATIONS = {
    # Progress and status messages
    "Preparing polar display...": "å‡†å¤‡æåæ ‡æ˜¾ç¤º...",
    "Preparing colored DTM": "å‡†å¤‡å½©è‰²DTM",
    "Please wait... reading in progress": "è¯·ç¨å€™...æ­£åœ¨è¯»å–",
    "Please wait... writing in progress": "è¯·ç¨å€™...æ­£åœ¨å†™å…¥",
    "Please wait... saving in progress": "è¯·ç¨å€™...æ­£åœ¨ä¿å­˜",
    "Computing entities scales": "è®¡ç®—å®ä½“æ¯”ä¾‹",
    "Triangulation in progress...": "ä¸‰è§’åŒ–è¿›è¡Œä¸­...",
    "Computing strain estimates": "è®¡ç®—åº”å˜ä¼°è®¡",
    "Calculating strain tensors...": "è®¡ç®—åº”å˜å¼ é‡...",
    "Estimating P21 Intensity": "ä¼°è®¡P21å¼ºåº¦",
    "Calculating patch areas...": "è®¡ç®—é¢ç‰‡åŒºåŸŸ...",
    
    # Error and warning messages with parameters
    "Missing parameter: filename after \"-%1\"": "ç¼ºå°‘å‚æ•°ï¼š\"-%1\" åçš„æ–‡ä»¶å",
    "Missing parameter: value after \"-%1\"": "ç¼ºå°‘å‚æ•°ï¼š\"-%1\" åçš„å€¼",
    "Missing parameter: vertices count after \"-%1\"": "ç¼ºå°‘å‚æ•°ï¼š\"-%1\" åçš„é¡¶ç‚¹æ•°é‡",
    "Missing parameter: extension after '%1'": "ç¼ºå°‘å‚æ•°ï¼š'%1' åçš„æ‰©å±•å",
    "Missing parameter: precision value after '%1'": "ç¼ºå°‘å‚æ•°ï¼š'%1' åçš„ç²¾åº¦å€¼",
    "Missing parameter: separator character after '%1'": "ç¼ºå°‘å‚æ•°ï¼š'%1' åçš„åˆ†éš”ç¬¦",
    "Unhandled format specifier (%1)": "æœªå¤„ç†çš„æ ¼å¼è¯´æ˜ç¬¦ (%1)",
    "Couldn't find the plugin '%1'": "æ‰¾ä¸åˆ°æ’ä»¶ '%1'",
    "Could not compute octree for cloud '%1'": "æ— æ³•ä¸ºç‚¹äº‘ '%1' è®¡ç®—å…«å‰æ ‘",
    "Third party library error: %1": "ç¬¬ä¸‰æ–¹åº“é”™è¯¯ï¼š%1",
    
    # Status messages with parameters
    "cloud %1/%2 (%3 points)": "ç‚¹äº‘ %1/%2ï¼ˆ%3 ä¸ªç‚¹ï¼‰",
    "Approximate number of points: %1": "è¿‘ä¼¼ç‚¹æ•°ï¼š%1",
    "Up to (%1 x %2 x %3) = %4 section(s)": "æœ€å¤š (%1 x %2 x %3) = %4 ä¸ªæˆªé¢",
    
    # Special messages
    "Hum, it seems that ECV has crashed... Sorry about that :)": "å—¯ï¼Œçœ‹èµ·æ¥ECVå´©æºƒäº†...å¯¹æ­¤æ„Ÿåˆ°æŠ±æ­‰ :)",
    "SmallWidgets Interface": "å°éƒ¨ä»¶ç•Œé¢",
    
    # File I/O messages
    "Can't save selected entity(ies) this way!": "æ— æ³•ä»¥è¿™ç§æ–¹å¼ä¿å­˜é€‰ä¸­çš„å®ä½“ï¼",
    "[I/O] The following selected entities won't be saved:": "[I/O] ä»¥ä¸‹é€‰ä¸­çš„å®ä½“ä¸ä¼šè¢«ä¿å­˜ï¼š",
    "\t- %1s": "\t- %1ç§’",
    "Some entities were ingored! (see console)": "æŸäº›å®ä½“è¢«å¿½ç•¥äº†ï¼ï¼ˆè§æ§åˆ¶å°ï¼‰",
    
    # Transformation messages  
    "Entity '%1' has been translated: (%2,%3,%4) and rescaled of a factor %5 [original position will be restored after saving]": 
        "å®ä½“ '%1' å·²è¢«å¹³ç§»ï¼š(%2,%3,%4) å¹¶æŒ‰å› å­ %5 é‡æ–°ç¼©æ”¾ [ä¿å­˜åå°†æ¢å¤åŸå§‹ä½ç½®]",
    "Resutling coordinates will be too big (original precision may be lost!). Proceed anyway?":
        "ç»“æœåæ ‡å°†å¤ªå¤§ï¼ˆåŸå§‹ç²¾åº¦å¯èƒ½ä¸¢å¤±ï¼ï¼‰ã€‚ä»è¦ç»§ç»­å—ï¼Ÿ",
    "Point (%1 ; %2 ; %3) set as rotation center for interactive transformation":
        "ç‚¹ (%1 ; %2 ; %3) è®¾ç½®ä¸ºäº¤äº’å˜æ¢çš„æ—‹è½¬ä¸­å¿ƒ",
    
    # Mesh operations messages
    "Full Screen 3D mode has not been implemented yet!": "å…¨å±3Dæ¨¡å¼å°šæœªå®ç°ï¼",
    "Only meshes with standard vertices are handled for now! Can't merge entity '%1'...":
        "ç›®å‰åªå¤„ç†å…·æœ‰æ ‡å‡†é¡¶ç‚¹çš„ç½‘æ ¼ï¼æ— æ³•åˆå¹¶å®ä½“ '%1'...",
    "Entity '%1' is neither a cloud nor a mesh, can't merge it!":
        "å®ä½“ '%1' æ—¢ä¸æ˜¯ç‚¹äº‘ä¹Ÿä¸æ˜¯ç½‘æ ¼ï¼Œæ— æ³•åˆå¹¶ï¼",
    "Can't mix point clouds and meshes!": "æ— æ³•æ··åˆç‚¹äº‘å’Œç½‘æ ¼ï¼",
    "Couldn't allocate a new scalar field for storing the original cloud index! Try to free some memory ...":
        "æ— æ³•åˆ†é…æ–°çš„æ ‡é‡åœºæ¥å­˜å‚¨åŸå§‹ç‚¹äº‘ç´¢å¼•ï¼å°è¯•é‡Šæ”¾ä¸€äº›å†…å­˜...",
    "Fusion failed! (not enough memory?)": "èåˆå¤±è´¥ï¼ï¼ˆå†…å­˜ä¸è¶³ï¼Ÿï¼‰",
    
    # Picking and interaction messages
    "Can't start the picking mechanism (another tool is already using it)":
        "æ— æ³•å¯åŠ¨æ‹¾å–æœºåˆ¶ï¼ˆå¦ä¸€ä¸ªå·¥å…·æ­£åœ¨ä½¿ç”¨å®ƒï¼‰",
    "[Level] Point is too close from the others!": "[æ°´å¹³ä»ª] ç‚¹è·å…¶ä»–ç‚¹å¤ªè¿‘ï¼",
    "Use best fit plane (yes) or the current viewing direction (no)":
        "ä½¿ç”¨æœ€ä½³æ‹Ÿåˆå¹³é¢ï¼ˆæ˜¯ï¼‰è¿˜æ˜¯å½“å‰æŸ¥çœ‹æ–¹å‘ï¼ˆå¦ï¼‰",
    
    # TBB message
    "[TBB] Using Intel's Threading Building Blocks %1.%2":
        "[TBB] ä½¿ç”¨Intelçš„çº¿ç¨‹æ„å»ºæ¨¡å— %1.%2",
    
    # ccCompass specific
    "MCMC Stride (radians):": "MCMCæ­¥å¹…ï¼ˆå¼§åº¦ï¼‰ï¼š",
    "The minimum size of the normal-estimation window.": "æ³•çº¿ä¼°è®¡çª—å£çš„æœ€å°å°ºå¯¸ã€‚",
    "The maximum size of the normal-estimation window.": "æ³•çº¿ä¼°è®¡çª—å£çš„æœ€å¤§å°ºå¯¸ã€‚",
    "Standard deviation of the normal distribution used to calculate monte-carlo jumps during sampling. Larger values lead to more exploration (and longer runtimes).":
        "ç”¨äºè®¡ç®—é‡‡æ ·æœŸé—´è’™ç‰¹å¡ç½—è·³è·ƒçš„æ­£æ€åˆ†å¸ƒçš„æ ‡å‡†åå·®ã€‚è¾ƒå¤§çš„å€¼å¯¼è‡´æ›´å¤šæ¢ç´¢ï¼ˆå’Œæ›´é•¿çš„è¿è¡Œæ—¶é—´ï¼‰ã€‚",
    "The voxel size for computing strain. This should be large enough that most boxes contain SNEs.":
        "ç”¨äºè®¡ç®—åº”å˜çš„ä½“ç´ å¤§å°ã€‚è¿™åº”è¯¥è¶³å¤Ÿå¤§ï¼Œä»¥ä¾¿å¤§å¤šæ•°ç›’å­åŒ…å«SNEã€‚",
    "Use SNE orientation estimates for outside the current cell if none are avaliable within it.":
        "å¦‚æœå½“å‰å•å…ƒæ ¼å†…æ²¡æœ‰SNEæ–¹å‘ä¼°è®¡ï¼Œåˆ™ä½¿ç”¨å¤–éƒ¨çš„ä¼°è®¡ã€‚",
    "Build graphic strain ellipses and grid domains. Useful for validation.":
        "æ„å»ºå›¾å½¢åº”å˜æ¤­åœ†å’Œç½‘æ ¼åŸŸã€‚å¯¹éªŒè¯æœ‰ç”¨ã€‚",
    "Exaggerate the shape of strain ellipses for easier visualisation.":
        "å¤¸å¤§åº”å˜æ¤­åœ†çš„å½¢çŠ¶ä»¥ä¾¿äºå¯è§†åŒ–ã€‚",
    "The search radius used to define the region to compute P21 within.":
        "ç”¨äºå®šä¹‰è®¡ç®—P21åŒºåŸŸçš„æœç´¢åŠå¾„ã€‚",
    "Only sample P21 on the each n'th point in the original outcrop model (decreases calculation time).":
        "ä»…åœ¨åŸå§‹éœ²å¤´æ¨¡å‹ä¸­çš„æ¯ç¬¬nä¸ªç‚¹ä¸Šé‡‡æ ·P21ï¼ˆå‡å°‘è®¡ç®—æ—¶é—´ï¼‰ã€‚",
    "CSV files (*.csv *.txt);XML (*.xml)": "CSVæ–‡ä»¶ (*.csv *.txt);XML (*.xml)",
    
    # Keyboard shortcuts (usually keep as-is, but add context)
    "Ctrl+O": "Ctrl+O",
    "Ctrl+S": "Ctrl+S",
    "Ctrl+Q": "Ctrl+Q",
    "Ctrl+P": "Ctrl+P",
    "Alt+B": "Alt+B",
    "Alt+C": "Alt+C",
    "Del": "Del",
    "5": "5",
    "4": "4",
    "6": "6",
    "7": "7",
    "8": "8",
    "9": "9",
    
    # RasterizeToolDialog
    "size of step of the grid generated (in the same units as the coordinates of the point cloud)":
        "ç”Ÿæˆç½‘æ ¼çš„æ­¥é•¿ï¼ˆä¸ç‚¹äº‘åæ ‡ç›¸åŒçš„å•ä½ï¼‰",
    "Active layer (or 'scalar field')": "æ´»åŠ¨å›¾å±‚ï¼ˆæˆ–\"æ ‡é‡åœº\"ï¼‰",
    "SF interpolation method": "æ ‡é‡åœºæ’å€¼æ–¹æ³•",
    "Use the nearest point of the input cloud in each cell instead of the cell center":
        "ä½¿ç”¨è¾“å…¥ç‚¹äº‘ä¸­æ¯ä¸ªå•å…ƒæ ¼çš„æœ€è¿‘ç‚¹ï¼Œè€Œä¸æ˜¯å•å…ƒæ ¼ä¸­å¿ƒ",
    "Per-cell height computation method:\n - minimum = lowest point in the cell\n - average = mean height of all points in the cell (distance to the 'average plane')\n - maximum = highest point in the cell":
        "æ¯å•å…ƒé«˜åº¦è®¡ç®—æ–¹æ³•ï¼š\n - æœ€å°å€¼ = å•å…ƒæ ¼ä¸­çš„æœ€ä½ç‚¹\n - å¹³å‡å€¼ = å•å…ƒæ ¼ä¸­æ‰€æœ‰ç‚¹çš„å¹³å‡é«˜åº¦ï¼ˆåˆ°\"å¹³å‡å¹³é¢\"çš„è·ç¦»ï¼‰\n - æœ€å¤§å€¼ = å•å…ƒæ ¼ä¸­çš„æœ€é«˜ç‚¹",
    "choose the value to fill the cells in which no point is projected : minimum value over the whole point cloud or NaN":
        "é€‰æ‹©å¡«å……æ²¡æœ‰ç‚¹æŠ•å½±çš„å•å…ƒæ ¼çš„å€¼ï¼šæ•´ä¸ªç‚¹äº‘çš„æœ€å°å€¼æˆ–NaN",
    "The contour plot is computed on the active layer": "è½®å»“å›¾åœ¨æ´»åŠ¨å›¾å±‚ä¸Šè®¡ç®—",
    "project contours on the altitude layer": "å°†è½®å»“æŠ•å½±åˆ°é«˜ç¨‹å›¾å±‚",
    "Hillshade is computed on the height layer": "å±±ä½“é˜´å½±åœ¨é«˜åº¦å›¾å±‚ä¸Šè®¡ç®—",
    "Zenith angle (in degrees) = 90 - altitude angle": "å¤©é¡¶è§’ï¼ˆåº¦ï¼‰= 90 - é«˜åº¦è§’",
    " deg.": " åº¦",
    
    # qCanupoPlugin
    "Load a previously saved classifier file.": "åŠ è½½å…ˆå‰ä¿å­˜çš„åˆ†ç±»å™¨æ–‡ä»¶ã€‚",
    "Save the current classifier to a file.": "å°†å½“å‰åˆ†ç±»å™¨ä¿å­˜åˆ°æ–‡ä»¶ã€‚",
    "Train a new classifier using the current core points.": "ä½¿ç”¨å½“å‰æ ¸å¿ƒç‚¹è®­ç»ƒæ–°åˆ†ç±»å™¨ã€‚",
    "Classify the point cloud using the current classifier.": "ä½¿ç”¨å½“å‰åˆ†ç±»å™¨å¯¹ç‚¹äº‘è¿›è¡Œåˆ†ç±»ã€‚",
    "Clear all core point clouds.": "æ¸…é™¤æ‰€æœ‰æ ¸å¿ƒç‚¹äº‘ã€‚",
    "Remove the selected core point cloud.": "ç§»é™¤é€‰ä¸­çš„æ ¸å¿ƒç‚¹äº‘ã€‚",
    "Add a new class and its core points.": "æ·»åŠ æ–°ç±»åˆ«åŠå…¶æ ¸å¿ƒç‚¹ã€‚",
    "The confidence threshold for classification.": "åˆ†ç±»çš„ç½®ä¿¡åº¦é˜ˆå€¼ã€‚",
    "Points with confidence below this threshold will be unclassified.": "ç½®ä¿¡åº¦ä½äºæ­¤é˜ˆå€¼çš„ç‚¹å°†ä¸è¢«åˆ†ç±»ã€‚",
    "Use the original cloud for descriptor computation.": "ä½¿ç”¨åŸå§‹ç‚¹äº‘è¿›è¡Œæè¿°ç¬¦è®¡ç®—ã€‚",
    "Set this cloud for descriptor computation.": "è®¾ç½®æ­¤ç‚¹äº‘ç”¨äºæè¿°ç¬¦è®¡ç®—ã€‚",
    "Multi-scale dimensionality descriptors.": "å¤šå°ºåº¦ç»´åº¦æè¿°ç¬¦ã€‚",
    "The scales at which to compute descriptors.": "è®¡ç®—æè¿°ç¬¦çš„å°ºåº¦ã€‚",
    "Smaller scales capture fine details.": "è¾ƒå°çš„å°ºåº¦æ•è·ç²¾ç»†ç»†èŠ‚ã€‚",
    "Larger scales capture broader features.": "è¾ƒå¤§çš„å°ºåº¦æ•è·æ›´å¹¿æ³›çš„ç‰¹å¾ã€‚",
    
    # DistanceMapGenerationDlg
    "Compute distance map": "è®¡ç®—è·ç¦»å›¾",
    "Distance computation type": "è·ç¦»è®¡ç®—ç±»å‹",
    "Signed distances (inside/outside)": "æœ‰ç¬¦å·è·ç¦»ï¼ˆå†…éƒ¨/å¤–éƒ¨ï¼‰",
    "Unsigned distances": "æ— ç¬¦å·è·ç¦»",
    "Euclidean distance": "æ¬§æ°è·ç¦»",
    "Manhattan distance": "æ›¼å“ˆé¡¿è·ç¦»",
    "Chebyshev distance": "åˆ‡æ¯”é›ªå¤«è·ç¦»",
    
    # M3C2Dialog
    "Core points cloud": "æ ¸å¿ƒç‚¹äº‘",
    "Normal scale": "æ³•çº¿å°ºåº¦",
    "Projection scale": "æŠ•å½±å°ºåº¦",
    "Max depth": "æœ€å¤§æ·±åº¦",
    "Cylindrical projection": "åœ†æŸ±æŠ•å½±",
    "Use cloud normals": "ä½¿ç”¨ç‚¹äº‘æ³•çº¿",
    "Compute normals": "è®¡ç®—æ³•çº¿",
    "Registration error": "é…å‡†è¯¯å·®",
    "Significance level": "æ˜¾è‘—æ€§æ°´å¹³",
    
    # RegistrationDialog
    "Reference cloud": "å‚è€ƒç‚¹äº‘",
    "Aligned cloud": "å¯¹é½ç‚¹äº‘",
    "Random sampling limit": "éšæœºé‡‡æ ·é™åˆ¶",
    "Final overlap": "æœ€ç»ˆé‡å ",
    "Use random sampling": "ä½¿ç”¨éšæœºé‡‡æ ·",
    "Adjust scale": "è°ƒæ•´æ¯”ä¾‹",
    "Number of iterations": "è¿­ä»£æ¬¡æ•°",
    "Convergence criterion": "æ”¶æ•›å‡†åˆ™",
    "Final RMS": "æœ€ç»ˆå‡æ–¹æ ¹",
    "Final transformation": "æœ€ç»ˆå˜æ¢",
    
    # VolumeCalcDialog
    "Ground level": "åœ°é¢é«˜ç¨‹",
    "Ceiling level": "é¡¶é¢é«˜ç¨‹",
    "Constant": "å¸¸é‡",
    "From cloud": "æ¥è‡ªç‚¹äº‘",
    "From mesh": "æ¥è‡ªç½‘æ ¼",
    "Volume report": "ä½“ç§¯æŠ¥å‘Š",
    "Volume above": "ä¸Šæ–¹ä½“ç§¯",
    "Volume below": "ä¸‹æ–¹ä½“ç§¯",
    "Total volume": "æ€»ä½“ç§¯",
    "Surface area": "è¡¨é¢ç§¯",
    "Average height": "å¹³å‡é«˜åº¦",
    
    # SACSegmentation  
    "Model type": "æ¨¡å‹ç±»å‹",
    "Sphere model": "çƒä½“æ¨¡å‹",
    "Cylinder model": "åœ†æŸ±æ¨¡å‹",
    "Cone model": "åœ†é”¥æ¨¡å‹",
    "Plane model": "å¹³é¢æ¨¡å‹",
    "Distance threshold": "è·ç¦»é˜ˆå€¼",
    "Max iterations": "æœ€å¤§è¿­ä»£æ¬¡æ•°",
    "Probability": "æ¦‚ç‡",
    "Extract inliers": "æå–å†…ç‚¹",
    "Extract outliers": "æå–å¤–ç‚¹",
    "Inlier count": "å†…ç‚¹æ•°é‡",
    "Model coefficients": "æ¨¡å‹ç³»æ•°",
    
    # qSRA
    "Profile comparison": "å‰–é¢æ¯”è¾ƒ",
    "Reference profile": "å‚è€ƒå‰–é¢",
    "Compare profile": "æ¯”è¾ƒå‰–é¢",
    "Roughness analysis": "ç²—ç³™åº¦åˆ†æ",
    "Profile length": "å‰–é¢é•¿åº¦",
    "Roughness index": "ç²—ç³™åº¦æŒ‡æ•°",
    
    # qRansacSD
    "Detect primitives": "æ£€æµ‹åŸºå…ƒ",
    "Primitive types": "åŸºå…ƒç±»å‹",
    "Min support points": "æœ€å°æ”¯æ’‘ç‚¹æ•°",
    "Sampling resolution": "é‡‡æ ·åˆ†è¾¨ç‡",
    "Max normal deviation": "æœ€å¤§æ³•çº¿åå·®",
    "Overlook probability": "å¿½ç•¥æ¦‚ç‡",
    "Detected shapes": "æ£€æµ‹åˆ°çš„å½¢çŠ¶",
    
    # DisplayOptionsDlg
    "Point size": "ç‚¹å¤§å°",
    "Line width": "çº¿å®½",
    "Default font": "é»˜è®¤å­—ä½“",
    "Label font size": "æ ‡ç­¾å­—ä½“å¤§å°",
    "Number precision": "æ•°å­—ç²¾åº¦",
    "Decimal places": "å°æ•°ä½æ•°",
    "Background color": "èƒŒæ™¯é¢œè‰²",
    "Text color": "æ–‡æœ¬é¢œè‰²",
    "Point color": "ç‚¹é¢œè‰²",
    "Line color": "çº¿é¢œè‰²",
    
    # GeomFeaturesDialog
    "Compute roughness": "è®¡ç®—ç²—ç³™åº¦",
    "Compute curvature": "è®¡ç®—æ›²ç‡",
    "Compute density": "è®¡ç®—å¯†åº¦",
    "Kernel radius": "æ ¸åŠå¾„",
    "Feature type": "ç‰¹å¾ç±»å‹",
    "Gaussian curvature": "é«˜æ–¯æ›²ç‡",
    "Mean curvature": "å¹³å‡æ›²ç‡",
    "Normal change rate": "æ³•çº¿å˜åŒ–ç‡",
    "Surface density": "è¡¨é¢å¯†åº¦",
    
    # InterpolationDlg
    "Interpolation method": "æ’å€¼æ–¹æ³•",
    "IDW (Inverse Distance Weighting)": "IDWï¼ˆåè·ç¦»åŠ æƒï¼‰",
    "Kriging": "å…‹é‡Œé‡‘",
    "Natural Neighbor": "è‡ªç„¶é‚»åŸŸ",
    "Power parameter": "å¹‚æ¬¡å‚æ•°",
    "Search radius": "æœç´¢åŠå¾„",
    "Min neighbors": "æœ€å°é‚»åŸŸæ•°",
    "Max neighbors": "æœ€å¤§é‚»åŸŸæ•°",
    "Interpolate scalar field": "æ’å€¼æ ‡é‡åœº",
    "Output grid": "è¾“å‡ºç½‘æ ¼",
    
    # PoissonReconParamDialog
    "Octree depth": "å…«å‰æ ‘æ·±åº¦",
    "Solver divide": "æ±‚è§£å™¨åˆ’åˆ†",
    "Samples per node": "æ¯èŠ‚ç‚¹é‡‡æ ·æ•°",
    "Point weight": "ç‚¹æƒé‡",
    "Trim threshold": "ä¿®å‰ªé˜ˆå€¼",
    "Linear fit": "çº¿æ€§æ‹Ÿåˆ",
    "Density threshold": "å¯†åº¦é˜ˆå€¼",
    "Boundary type": "è¾¹ç•Œç±»å‹",
    "Free boundary": "è‡ªç”±è¾¹ç•Œ",
    "Dirichlet boundary": "ç‹„åˆ©å…‹é›·è¾¹ç•Œ",
    "Neumann boundary": "è¯ºä¼Šæ›¼è¾¹ç•Œ",
    
    # qHPR
    "Hidden point removal": "éšè—ç‚¹ç§»é™¤",
    "Camera position": "ç›¸æœºä½ç½®",
    "Camera radius": "ç›¸æœºåŠå¾„",
    "Remove hidden": "ç§»é™¤éšè—ç‚¹",
    "Keep visible": "ä¿ç•™å¯è§ç‚¹",
    "Visible points": "å¯è§ç‚¹",
    "Hidden points": "éšè—ç‚¹",
}

def translate_ts_file_round2(input_file, output_file):
    """Round 2 translation focusing on complete sentences and messages"""
    
    tree = ET.parse(input_file)
    root = tree.getroot()
    
    translated_count = 0
    still_untranslated = []
    
    for context in root.findall('.//context'):
        context_name = context.find('name').text if context.find('name') is not None else "Unknown"
        
        for message in context.findall('message'):
            translation = message.find('translation')
            if translation is None:
                continue
            
            # Only process unfinished
            trans_type = translation.get('type', '')
            if trans_type != 'unfinished' and translation.text:
                continue
            
            source = message.find('source')
            if source is None or not source.text:
                continue
            
            source_text = source.text
            
            # Check exact match
            if source_text in ROUND2_TRANSLATIONS:
                translation.text = ROUND2_TRANSLATIONS[source_text]
                if 'type' in translation.attrib:
                    del translation.attrib['type']
                translated_count += 1
            else:
                still_untranslated.append({
                    'context': context_name,
                    'source': source_text
                })
    
    # Write output
    tree.write(output_file, encoding='utf-8', xml_declaration=True)
    
    # Save remaining for round 3
    with open('/home/ludahai/develop/code/github/ACloudViewer/eCV/translations/scripts/remaining_round3.txt',
              'w', encoding='utf-8') as f:
        f.write(f"ç¬¬3è½®å¾…ç¿»è¯‘å†…å®¹ ({len(still_untranslated)} æ¡)\n")
        f.write(f"{'='*80}\n\n")
        
        from collections import defaultdict
        by_context = defaultdict(list)
        for item in still_untranslated:
            by_context[item['context']].append(item['source'])
        
        for ctx, items in sorted(by_context.items(), key=lambda x: len(x[1]), reverse=True):
            f.write(f"\n{ctx} ({len(items)} æ¡)\n")
            f.write(f"{'-'*80}\n")
            for i, source in enumerate(items[:30], 1):
                preview = source[:100].replace('\n', ' ')
                f.write(f"{i}. {preview}\n")
            if len(items) > 30:
                f.write(f"... è¿˜æœ‰ {len(items)-30} æ¡\n")
    
    print(f"\n{'='*80}")
    print(f"ç¬¬2è½®ç¿»è¯‘å®Œæˆï¼")
    print(f"{'='*80}")
    print(f"æœ¬æ¬¡ç¿»è¯‘: {translated_count} æ¡")
    print(f"ä»éœ€ç¿»è¯‘: {len(still_untranslated)} æ¡")
    print(f"è¯¦ç»†åˆ—è¡¨: scripts/remaining_round3.txt")
    print(f"{'='*80}\n")
    
    return translated_count, len(still_untranslated)

if __name__ == "__main__":
    input_file = '/home/ludahai/develop/code/github/ACloudViewer/eCV/translations/ACloudViewer_zh.ts'
    output_file = input_file
    
    translated, remaining = translate_ts_file_round2(input_file, output_file)
    
    print(f"âœ“ ç¬¬2è½®æˆåŠŸç¿»è¯‘ {translated} æ¡")
    if remaining > 0:
        print(f"! è¿˜æœ‰ {remaining} æ¡éœ€è¦ç¬¬3è½®å¤„ç†")
    else:
        print(f"ğŸ‰ 100%è¦†ç›–ç‡è¾¾æˆï¼")
