#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¿»è¯‘è´¨é‡å®¡æ ¸ä¸æ”¹è¿›è„šæœ¬
ç¡®ä¿ç¿»è¯‘ç¬¦åˆ"ä¿¡è¾¾é›…"æ ‡å‡†ï¼Œæœ¯è¯­ç»Ÿä¸€ï¼Œè¡¨è¾¾æ— æ­§ä¹‰
"""

import xml.etree.ElementTree as ET
import re
from collections import defaultdict

# æœ¯è¯­ç»Ÿä¸€è§„èŒƒ - åŸºäºTRANSLATION_STANDARDS.md
TERMINOLOGY_STANDARDS = {
    # éœ€è¦ç»Ÿä¸€çš„æœ¯è¯­å¯¹ï¼ˆé”™è¯¯ -> æ­£ç¡®ï¼‰
    "äº‘ç‚¹": "ç‚¹äº‘",
    "æ ¼ç½‘": "ç½‘æ ¼",
    "æ³•å‘é‡": "æ³•çº¿",
    "æ³¨å†Œ": "é…å‡†",
    "ç¦»ç¾¤ç‚¹": "å¼‚å¸¸å€¼",
    "é‡å€¼": "å¼‚å¸¸å€¼",
    "å™ªéŸ³": "å™ªå£°",
    "è¾¹ç•Œæ¡†": "åŒ…å›´ç›’",
    "æ‘„åƒæœº": "ç›¸æœº",
    "ææ–™": "æè´¨",
    "æ•£åˆ—": "å“ˆå¸Œ",
    "æ ‡è®°": "æ ‡ç­¾",
    "é‡æ„": "é‡å»º",
    "æ‰§è¡Œ": "åº”ç”¨",
    "å¯¹é½": "é…å‡†",  # åœ¨Registrationä¸Šä¸‹æ–‡ä¸­
}

# éœ€è¦æ£€æŸ¥ä¸€è‡´æ€§çš„æœ¯è¯­ï¼ˆåŒä¸€æ¦‚å¿µåº”è¯¥åªç”¨ä¸€ç§ç¿»è¯‘ï¼‰
CONSISTENCY_CHECK = {
    "Point Cloud": ["ç‚¹äº‘"],
    "Mesh": ["ç½‘æ ¼"],
    "Normal": ["æ³•çº¿"],
    "Scalar Field": ["æ ‡é‡åœº"],
    "Registration": ["é…å‡†"],
    "Bounding Box": ["åŒ…å›´ç›’"],
    "Filter": ["æ»¤æ³¢"],
    "Segment": ["åˆ†å‰²"],
    "Extract": ["æå–"],
    "Transform": ["å˜æ¢"],
    "Rotation": ["æ—‹è½¬"],
    "Translation": ["å¹³ç§»"],
    "Scale": ["ç¼©æ”¾"],
}

# éœ€è¦æ”¹è¿›è¡¨è¾¾çš„æ¨¡å¼ï¼ˆç›´è¯‘ -> ä¿¡è¾¾é›…ï¼‰
EXPRESSION_IMPROVEMENTS = {
    # æ”¹è¿›å†—é•¿æˆ–ä¸è‡ªç„¶çš„è¡¨è¾¾
    "è¿›è¡Œ...æ“ä½œ": "...",  # å»é™¤å†—ä½™
    "å®æ–½...": "æ‰§è¡Œ...",
    "æ‰§è¡Œè®¡ç®—": "è®¡ç®—",
    "æ‰§è¡Œæ“ä½œ": "æ“ä½œ",
    
    # æ”¹è¿›è¢«åŠ¨è¯­æ€ä¸ºä¸»åŠ¨è¯­æ€ï¼ˆç¬¦åˆä¸­æ–‡ä¹ æƒ¯ï¼‰
    "è¢«é€‰æ‹©çš„": "æ‰€é€‰çš„",
    "è¢«è®¡ç®—çš„": "å·²è®¡ç®—çš„",
    "å°†è¢«": "å°†",
    
    # æ”¹è¿›æ¬§åŒ–å¥å¼
    "å®ƒæ˜¯": "è¿™æ˜¯",
    "è¿™ä¸ªæ˜¯": "è¿™æ˜¯",
    "é‚£ä¸ªæ˜¯": "é‚£æ˜¯",
}

# æ­§ä¹‰è¡¨è¾¾æ£€æŸ¥ï¼ˆå¯èƒ½äº§ç”Ÿæ­§ä¹‰çš„è¯æ±‡ï¼‰
AMBIGUOUS_TERMS = {
    "å¤„ç†": ["æ»¤æ³¢", "è®¡ç®—", "æ“ä½œ"],  # "å¤„ç†"å¤ªæ³›ï¼Œåº”å…·ä½“åŒ–
    "èŠ‚ç‚¹": ["é¡¶ç‚¹", "èŠ‚ç‚¹"],  # éœ€è¦åŒºåˆ†å‡ ä½•é¡¶ç‚¹å’Œæ•°æ®ç»“æ„èŠ‚ç‚¹
    "ç‚¹": ["ç‚¹", "é¡¶ç‚¹"],  # éœ€è¦åŒºåˆ†Pointå’ŒVertex
    "é¢": ["é¢", "å¹³é¢"],  # éœ€è¦åŒºåˆ†Faceå’ŒPlane
}


def check_terminology_consistency(text: str) -> list:
    """æ£€æŸ¥æœ¯è¯­ä½¿ç”¨æ˜¯å¦ç¬¦åˆè§„èŒƒ"""
    issues = []
    
    for wrong_term, correct_term in TERMINOLOGY_STANDARDS.items():
        if wrong_term in text:
            issues.append({
                'type': 'terminology',
                'severity': 'high',
                'message': f'ä½¿ç”¨äº†éæ ‡å‡†æœ¯è¯­ "{wrong_term}"ï¼Œåº”æ”¹ä¸º "{correct_term}"',
                'suggestion': text.replace(wrong_term, correct_term)
            })
    
    return issues


def check_expression_quality(text: str) -> list:
    """æ£€æŸ¥è¡¨è¾¾è´¨é‡ï¼ˆä¿¡è¾¾é›…ï¼‰"""
    issues = []
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç›´è¯‘ç—•è¿¹
    for pattern, improvement in EXPRESSION_IMPROVEMENTS.items():
        if pattern in text:
            issues.append({
                'type': 'expression',
                'severity': 'medium',
                'message': f'è¡¨è¾¾å¯ä»¥æ”¹è¿›ï¼š"{pattern}" -> "{improvement}"',
                'suggestion': text.replace(pattern, improvement)
            })
    
    # æ£€æŸ¥å¥å­é•¿åº¦ï¼ˆä¸­æ–‡å¥å­ä¸å®œè¿‡é•¿ï¼‰
    if len(text) > 100 and 'ï¼Œ' not in text[-50:]:
        issues.append({
            'type': 'readability',
            'severity': 'low',
            'message': 'å¥å­è¾ƒé•¿ï¼Œå»ºè®®é€‚å½“æ–­å¥',
            'suggestion': None
        })
    
    return issues


def check_ambiguity(text: str) -> list:
    """æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ­§ä¹‰"""
    issues = []
    
    for ambiguous, alternatives in AMBIGUOUS_TERMS.items():
        if ambiguous in text:
            issues.append({
                'type': 'ambiguity',
                'severity': 'medium',
                'message': f'"{ambiguous}"å¯èƒ½äº§ç”Ÿæ­§ä¹‰ï¼Œå»ºè®®æ˜ç¡®ä¸ºï¼š{", ".join(alternatives)}',
                'suggestion': None
            })
    
    return issues


def audit_translation_quality(ts_file: str) -> dict:
    """å®¡æ ¸ç¿»è¯‘æ–‡ä»¶è´¨é‡"""
    
    tree = ET.parse(ts_file)
    root = tree.getroot()
    
    audit_results = {
        'total_messages': 0,
        'audited_messages': 0,
        'issues': [],
        'by_context': defaultdict(list),
        'by_type': defaultdict(int),
        'by_severity': defaultdict(int),
    }
    
    for context in root.findall('context'):
        context_name = context.find('name').text or "Unknown"
        
        for message in context.findall('message'):
            audit_results['total_messages'] += 1
            
            source = message.find('source')
            translation = message.find('translation')
            
            if source is None or translation is None:
                continue
            
            src_text = source.text or ""
            trans_text = translation.text or ""
            
            # åªå®¡æ ¸å·²ç¿»è¯‘çš„å†…å®¹
            if not trans_text:
                continue
            
            audit_results['audited_messages'] += 1
            
            # æ‰§è¡Œå„é¡¹æ£€æŸ¥
            all_issues = []
            all_issues.extend(check_terminology_consistency(trans_text))
            all_issues.extend(check_expression_quality(trans_text))
            all_issues.extend(check_ambiguity(trans_text))
            
            if all_issues:
                for issue in all_issues:
                    issue['context'] = context_name
                    issue['source'] = src_text
                    issue['translation'] = trans_text
                    
                    audit_results['issues'].append(issue)
                    audit_results['by_context'][context_name].append(issue)
                    audit_results['by_type'][issue['type']] += 1
                    audit_results['by_severity'][issue['severity']] += 1
    
    return audit_results


def apply_improvements(ts_file: str, output_file: str) -> dict:
    """åº”ç”¨ç¿»è¯‘æ”¹è¿›"""
    
    tree = ET.parse(ts_file)
    root = tree.getroot()
    
    stats = {
        'total': 0,
        'improved': 0,
        'unchanged': 0,
        'improvements': []
    }
    
    for context in root.findall('context'):
        context_name = context.find('name').text or ""
        
        for message in context.findall('message'):
            stats['total'] += 1
            
            source = message.find('source')
            translation = message.find('translation')
            
            if source is None or translation is None:
                continue
            
            src_text = source.text or ""
            trans_text = translation.text or ""
            
            if not trans_text:
                stats['unchanged'] += 1
                continue
            
            # åº”ç”¨æœ¯è¯­ç»Ÿä¸€æ”¹è¿›
            improved_text = trans_text
            changed = False
            
            for wrong_term, correct_term in TERMINOLOGY_STANDARDS.items():
                if wrong_term in improved_text:
                    improved_text = improved_text.replace(wrong_term, correct_term)
                    changed = True
            
            # åº”ç”¨è¡¨è¾¾æ”¹è¿›
            for pattern, improvement in EXPRESSION_IMPROVEMENTS.items():
                if pattern in improved_text:
                    improved_text = improved_text.replace(pattern, improvement)
                    changed = True
            
            if changed:
                translation.text = improved_text
                stats['improved'] += 1
                stats['improvements'].append({
                    'context': context_name,
                    'source': src_text[:80],
                    'before': trans_text,
                    'after': improved_text
                })
            else:
                stats['unchanged'] += 1
    
    # å†™å…¥è¾“å‡ºæ–‡ä»¶
    tree.write(output_file, encoding='utf-8', xml_declaration=True)
    
    # ä¿®å¤DOCTYPE
    with open(output_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    if '<!DOCTYPE TS>' not in content:
        content = content.replace(
            '<?xml version=\'1.0\' encoding=\'utf-8\'?>',
            '<?xml version="1.0" encoding="utf-8"?>\n<!DOCTYPE TS>'
        )
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(content)
    
    return stats


def generate_audit_report(audit_results: dict, output_file: str):
    """ç”Ÿæˆå®¡æ ¸æŠ¥å‘Š"""
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("# ç¿»è¯‘è´¨é‡å®¡æ ¸æŠ¥å‘Š\n\n")
        f.write(f"**å®¡æ ¸æ—¥æœŸ**: 2026-01-14\n\n")
        
        # æ€»è§ˆ
        f.write("## å®¡æ ¸æ€»è§ˆ\n\n")
        f.write(f"- æ€»æ¶ˆæ¯æ•°: {audit_results['total_messages']}\n")
        f.write(f"- å·²å®¡æ ¸: {audit_results['audited_messages']}\n")
        f.write(f"- å‘ç°é—®é¢˜: {len(audit_results['issues'])}\n\n")
        
        # æŒ‰ç±»å‹ç»Ÿè®¡
        f.write("## é—®é¢˜ç±»å‹åˆ†å¸ƒ\n\n")
        f.write("| ç±»å‹ | æ•°é‡ |\n")
        f.write("|------|------|\n")
        for issue_type, count in sorted(audit_results['by_type'].items()):
            f.write(f"| {issue_type} | {count} |\n")
        f.write("\n")
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦ç»Ÿè®¡
        f.write("## é—®é¢˜ä¸¥é‡ç¨‹åº¦\n\n")
        f.write("| ä¸¥é‡ç¨‹åº¦ | æ•°é‡ |\n")
        f.write("|----------|------|\n")
        for severity, count in sorted(audit_results['by_severity'].items()):
            f.write(f"| {severity} | {count} |\n")
        f.write("\n")
        
        # æŒ‰ä¸Šä¸‹æ–‡ç»Ÿè®¡ï¼ˆå‰10ä¸ªï¼‰
        f.write("## é—®é¢˜æœ€å¤šçš„ç»„ä»¶ï¼ˆå‰10ä¸ªï¼‰\n\n")
        f.write("| ç»„ä»¶ | é—®é¢˜æ•° |\n")
        f.write("|------|--------|\n")
        sorted_contexts = sorted(
            audit_results['by_context'].items(),
            key=lambda x: len(x[1]),
            reverse=True
        )
        for context, issues in sorted_contexts[:10]:
            f.write(f"| {context} | {len(issues)} |\n")
        f.write("\n")
        
        # è¯¦ç»†é—®é¢˜åˆ—è¡¨ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰
        f.write("## é«˜ä¼˜å…ˆçº§é—®é¢˜è¯¦æƒ…\n\n")
        high_priority = [i for i in audit_results['issues'] if i['severity'] == 'high']
        if high_priority:
            for idx, issue in enumerate(high_priority[:20], 1):
                f.write(f"### {idx}. {issue['context']}\n\n")
                f.write(f"**åŸæ–‡**: {issue['source'][:100]}\n\n")
                f.write(f"**å½“å‰ç¿»è¯‘**: {issue['translation']}\n\n")
                f.write(f"**é—®é¢˜**: {issue['message']}\n\n")
                if issue['suggestion']:
                    f.write(f"**å»ºè®®**: {issue['suggestion']}\n\n")
                f.write("---\n\n")
        else:
            f.write("âœ… æœªå‘ç°é«˜ä¼˜å…ˆçº§é—®é¢˜\n\n")
        
        # å»ºè®®
        f.write("## æ”¹è¿›å»ºè®®\n\n")
        f.write("1. ä¼˜å…ˆä¿®å¤é«˜ä¼˜å…ˆçº§é—®é¢˜\n")
        f.write("2. ç»Ÿä¸€æœ¯è¯­ä½¿ç”¨\n")
        f.write("3. æ”¹è¿›è¡¨è¾¾æµç•…åº¦\n")
        f.write("4. æ¶ˆé™¤æ­§ä¹‰è¡¨è¾¾\n")
        f.write("5. å‚è€ƒ TRANSLATION_STANDARDS.md è¿›è¡Œè§„èŒƒåŒ–\n")


def main():
    import sys
    
    ts_file = "/home/ludahai/develop/code/github/ACloudViewer/eCV/translations/ACloudViewer_zh.ts"
    
    print("=" * 80)
    print("ç¿»è¯‘è´¨é‡å®¡æ ¸ä¸æ”¹è¿›")
    print("=" * 80)
    print()
    
    # 1. å®¡æ ¸ç°æœ‰ç¿»è¯‘
    print("ğŸ“Š æ­£åœ¨å®¡æ ¸ç¿»è¯‘è´¨é‡...")
    audit_results = audit_translation_quality(ts_file)
    
    print(f"\nå®¡æ ¸å®Œæˆ:")
    print(f"  æ€»æ¶ˆæ¯æ•°: {audit_results['total_messages']}")
    print(f"  å·²å®¡æ ¸: {audit_results['audited_messages']}")
    print(f"  å‘ç°é—®é¢˜: {len(audit_results['issues'])}")
    
    if audit_results['issues']:
        print(f"\né—®é¢˜åˆ†å¸ƒ:")
        for issue_type, count in audit_results['by_type'].items():
            print(f"  - {issue_type}: {count}")
        
        print(f"\nä¸¥é‡ç¨‹åº¦:")
        for severity, count in audit_results['by_severity'].items():
            print(f"  - {severity}: {count}")
    
    # 2. ç”Ÿæˆå®¡æ ¸æŠ¥å‘Š
    report_file = "/home/ludahai/develop/code/github/ACloudViewer/eCV/translations/QUALITY_AUDIT_REPORT.md"
    print(f"\nğŸ“ ç”Ÿæˆå®¡æ ¸æŠ¥å‘Š: {report_file}")
    generate_audit_report(audit_results, report_file)
    
    # 3. åº”ç”¨è‡ªåŠ¨æ”¹è¿›
    print(f"\nğŸ”§ åº”ç”¨è‡ªåŠ¨æ”¹è¿›...")
    output_file = ts_file  # ç›´æ¥æ”¹è¿›åŸæ–‡ä»¶ï¼ˆå·²æœ‰å¤‡ä»½ï¼‰
    stats = apply_improvements(ts_file, output_file)
    
    print(f"\næ”¹è¿›ç»Ÿè®¡:")
    print(f"  æ€»è®¡: {stats['total']}")
    print(f"  å·²æ”¹è¿›: {stats['improved']}")
    print(f"  æœªå˜æ›´: {stats['unchanged']}")
    
    if stats['improvements']:
        print(f"\næ”¹è¿›ç¤ºä¾‹ï¼ˆå‰5ä¸ªï¼‰:")
        for idx, imp in enumerate(stats['improvements'][:5], 1):
            print(f"\n  {idx}. [{imp['context']}]")
            print(f"     åŸæ–‡: {imp['source']}...")
            print(f"     æ”¹å‰: {imp['before']}")
            print(f"     æ”¹å: {imp['after']}")
    
    print("\n" + "=" * 80)
    print("âœ… å®¡æ ¸ä¸æ”¹è¿›å®Œæˆï¼")
    print("=" * 80)
    print(f"\nğŸ“„ æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š: {report_file}")
    print(f"ğŸ“˜ å‚è€ƒç¿»è¯‘è§„èŒƒ: eCV/translations/TRANSLATION_STANDARDS.md")


if __name__ == "__main__":
    main()
